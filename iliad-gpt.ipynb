{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10154413,"sourceType":"datasetVersion","datasetId":6206145}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:32:30.338755Z","iopub.execute_input":"2024-12-10T12:32:30.339096Z","iopub.status.idle":"2024-12-10T12:32:30.685794Z","shell.execute_reply.started":"2024-12-10T12:32:30.339069Z","shell.execute_reply":"2024-12-10T12:32:30.684954Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/iliad-gutenberg/synthetic_qa_dataset.json\n/kaggle/input/iliad-gutenberg/Iliad.txt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Problem Description\n    - Picture a student worried sick about their big literature exam tomorrow morning. They read the novel and looked at SparkNotes but still don't feel confident.\n    - Personally, I think a conversation is the best way to engage and prepare for such an exam. BUt not everyone has an expert they can freely consult for an in-depth study session.\n    - LLMs offer the perfect tool to chat about a book and clarify any concerns about the text. \n    - I will showcase attempts at solving this use case through a look at Homer's 'The Iliad'.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:32:32.501596Z","iopub.execute_input":"2024-12-10T12:32:32.501994Z","iopub.status.idle":"2024-12-10T12:32:35.710298Z","shell.execute_reply.started":"2024-12-10T12:32:32.501969Z","shell.execute_reply":"2024-12-10T12:32:35.709621Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:32:37.215822Z","iopub.execute_input":"2024-12-10T12:32:37.216285Z","iopub.status.idle":"2024-12-10T12:32:37.251954Z","shell.execute_reply.started":"2024-12-10T12:32:37.216253Z","shell.execute_reply":"2024-12-10T12:32:37.250952Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## EDA on 'The Iliad'\n    - Homer's The Iliad is an ancient Greek epic poem that recounts a critical period during the Trojan War, focusing on the wrath of Achilles and its devastating consequences. It explores themes of heroism, fate, honor, and the human cost of war. As one of the oldest works in Western literature, it provides insight into ancient Greek culture and values while influencing countless works of art and literature. Its enduring importance lies in its timeless exploration of universal human experiences like anger, loss, and glory.\n    - open source on project gutenburg https://www.gutenberg.org/ebooks/6130\n    - Need to clean the text down since the intro and end contain legal jargin we don't want the model to see during training\n    - Will also want to investigate the length of the text","metadata":{}},{"cell_type":"code","source":"# Read the corpus\nwith open(\"/kaggle/input/iliad-gutenberg/Iliad.txt\", \"r\", encoding=\"utf-8\") as f:\n    iliad = f.read()\n\nprint(len(iliad))\nprint(iliad[:500])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:32:39.856574Z","iopub.execute_input":"2024-12-10T12:32:39.856918Z","iopub.status.idle":"2024-12-10T12:32:39.888421Z","shell.execute_reply.started":"2024-12-10T12:32:39.856889Z","shell.execute_reply":"2024-12-10T12:32:39.887545Z"}},"outputs":[{"name":"stdout","text":"1116791\nThe Project Gutenberg eBook of The Iliad\n    \nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United States,\nyou will have to check the laws of the country where you are located\nbefore using this eBook.\n\nTitle: The\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"START_STR = \"*** START OF THE PROJECT GUTENBERG EBOOK THE ILIAD ***\"\nEND_STR = \"*** END OF THE PROJECT GUTENBERG EBOOK THE ILIAD ***\"\n\ngutenburg_intro_index = iliad.find(START_STR) + len(START_STR)\ngutenburg_legal_terms_index = iliad.find(END_STR)\n\niliad = iliad[gutenburg_intro_index:gutenburg_legal_terms_index].strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:32:41.692281Z","iopub.execute_input":"2024-12-10T12:32:41.693023Z","iopub.status.idle":"2024-12-10T12:32:41.699028Z","shell.execute_reply.started":"2024-12-10T12:32:41.692987Z","shell.execute_reply":"2024-12-10T12:32:41.698030Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(len(iliad))\niliad[:500]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:32:42.630217Z","iopub.execute_input":"2024-12-10T12:32:42.631085Z","iopub.status.idle":"2024-12-10T12:32:42.638768Z","shell.execute_reply.started":"2024-12-10T12:32:42.631039Z","shell.execute_reply":"2024-12-10T12:32:42.637770Z"}},"outputs":[{"name":"stdout","text":"1097444\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'The\\nIliad of Homer\\n\\nTranslated by\\nAlexander Pope,\\n\\nWith Notes and Introduction\\nby the\\nRev. Theodore Alois Buckley, M.A., F.S.A.\\n\\nand\\nFlaxman’s Designs.\\n\\n1899\\n\\n\\nContents\\n\\n INTRODUCTION.\\n POPE’S PREFACE TO THE ILIAD OF HOMER\\n\\n THE ILIAD\\n BOOK I.\\n BOOK II.\\n BOOK III.\\n BOOK IV.\\n BOOK V.\\n BOOK VI.\\n BOOK VII.\\n BOOK VIII.\\n BOOK IX.\\n BOOK X.\\n BOOK XI.\\n BOOK XII.\\n BOOK XIII.\\n BOOK XIV.\\n BOOK XV.\\n BOOK XVI.\\n BOOK XVII.\\n BOOK XVIII.\\n BOOK XIX.\\n BOOK XX.\\n BOOK XXI.\\n BOOK XXII.\\n BOOK XXIII.\\n BOOK XXIV.\\n\\n CON'"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Byte Pair Encoding\n    - Byte Pair Encoding (BPE) is a subword tokenization technique used to efficiently handle text by splitting words into smaller, more frequent units. It starts with individual characters and iteratively merges the most common adjacent pairs of tokens to form subwords. This method balances vocabulary size and model performance by capturing both common words and rare subword patterns. BPE is widely used in modern NLP models like GPT to handle out-of-vocabulary words and improve generalization.\n    - BPE is the tokenizer used for many GPT models\n    - Rather than training my own BPE tokenizer, I will use the publicly available version from GPT-2 through tiktoken\n    - tiktoken docs: https://github.com/openai/tiktoken","metadata":{}},{"cell_type":"code","source":"!pip install tiktoken","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:32:43.820051Z","iopub.execute_input":"2024-12-10T12:32:43.820387Z","iopub.status.idle":"2024-12-10T12:32:53.250408Z","shell.execute_reply.started":"2024-12-10T12:32:43.820357Z","shell.execute_reply":"2024-12-10T12:32:53.249448Z"}},"outputs":[{"name":"stdout","text":"Collecting tiktoken\n  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\nDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.8.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import tiktoken\n\nbpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n\nprint(bpe_tokenizer.encode(\"hello world\"))\nassert bpe_tokenizer.decode(bpe_tokenizer.encode(\"hello world\")) == \"hello world\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:32:54.318135Z","iopub.execute_input":"2024-12-10T12:32:54.318467Z","iopub.status.idle":"2024-12-10T12:32:58.609025Z","shell.execute_reply.started":"2024-12-10T12:32:54.318440Z","shell.execute_reply":"2024-12-10T12:32:58.608093Z"}},"outputs":[{"name":"stdout","text":"[31373, 995]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"encoded_iliad = bpe_tokenizer.encode(iliad)\nprint(len(encoded_iliad))\nprint(encoded_iliad[:50])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:32:58.610441Z","iopub.execute_input":"2024-12-10T12:32:58.610755Z","iopub.status.idle":"2024-12-10T12:32:58.752121Z","shell.execute_reply.started":"2024-12-10T12:32:58.610729Z","shell.execute_reply":"2024-12-10T12:32:58.751286Z"}},"outputs":[{"name":"stdout","text":"311085\n[464, 198, 40, 4528, 324, 286, 28440, 198, 198, 8291, 17249, 416, 198, 38708, 13258, 11, 198, 198, 3152, 11822, 290, 22395, 198, 1525, 262, 198, 18009, 13, 36494, 978, 10924, 41493, 11, 337, 13, 32, 1539, 376, 13, 50, 13, 32, 13, 198, 198, 392, 198, 7414, 897, 805]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Dataset & Dataloader\n    - GPT models are causal models trained on the task of next work prediction\n    - Also, GPT models are auto regressive through the attention mechanism, meaning past tokens will be used to influence next token prediction. This yields the question, how many past tokens will be used to influence the next prediction? The 'context-window' indicates the number of tokens used to influence the prediction. A larger context window will require a larger model architecture, but will allow a better memory for the model when generating text. I'll discuss why this increases the architecture size later, but for now this is enough info to operate on.\n    - In summary, we need our dataset to allow us to pull blocks of tokens less than or equal to our context window size, and we need to be able to do next token prediction. This will be achieved through the __getitem__ function in a pytorch Dataset. Here's an example of the expected output from __getitem__:\n    \n    tokens = [11, 22, 34, 1, 25, 98, 5]\n    context_window = 3\n    \n    __getitem__(1) returns:\n        [22, 34, 1], [34, 1, 25]\n\n    - By making sure the target tensor is just the input selection shifted forward by 1 index, we can perform next token prediction at each index in the input tensor! This provides many more training examples for the model. Fruther we ensure text is the length of the context window so that context window is adhered.\n\n    - The DataLoader will use the dataset to serve batches of training examples from the dataset. We'll shuffle the dataset as well to further introduce randomness to the learning process of stocastic gradient descent.","metadata":{}},{"cell_type":"code","source":"class GPTDataset(Dataset):\n    def __init__(self, tokens: list[int], context_window: int):\n        self.tokens = tokens\n        self.context_window = context_window\n        self.num_tokens = len(self.tokens)\n        \n    def __len__(self):\n        return self.num_tokens // self.context_window\n    \n    def __getitem__(self, idx):\n        # Get a slice of the tokens from the dataset\n        start_idx = idx\n        end_idx = start_idx + self.context_window\n        \n        # The input is the tokens from start to end-1, and the target is from start+1 to end\n        input_tokens = self.tokens[start_idx:end_idx]\n        target_tokens = self.tokens[start_idx + 1:end_idx + 1]  # Predict next token\n        \n        # Convert tokens to tensor\n        input_tensor = torch.tensor(input_tokens)\n        target_tensor = torch.tensor(target_tokens)\n        \n        return input_tensor, target_tensor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:33:00.995122Z","iopub.execute_input":"2024-12-10T12:33:00.996081Z","iopub.status.idle":"2024-12-10T12:33:01.001503Z","shell.execute_reply.started":"2024-12-10T12:33:00.996041Z","shell.execute_reply":"2024-12-10T12:33:01.000734Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def create_dataloader(tokens: list[int], context_window: int, batch_size: int=4, shuffle: bool=True):\n    dataset = GPTDataset(tokens, context_window)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle) \n    return dataloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:33:01.908437Z","iopub.execute_input":"2024-12-10T12:33:01.909241Z","iopub.status.idle":"2024-12-10T12:33:01.913344Z","shell.execute_reply.started":"2024-12-10T12:33:01.909199Z","shell.execute_reply":"2024-12-10T12:33:01.912568Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"CONTEXT_WINDOW: int = 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:33:02.923514Z","iopub.execute_input":"2024-12-10T12:33:02.924299Z","iopub.status.idle":"2024-12-10T12:33:02.927935Z","shell.execute_reply.started":"2024-12-10T12:33:02.924266Z","shell.execute_reply":"2024-12-10T12:33:02.927050Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"dataloader = create_dataloader(encoded_iliad, CONTEXT_WINDOW)\n\nfor input_tensor, target_tensor in dataloader:\n    print(f\"Input: {input_tensor.shape}\")\n    print(f\"Target: {target_tensor.shape}\")\n    break  # Just show the first batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:33:03.833109Z","iopub.execute_input":"2024-12-10T12:33:03.833444Z","iopub.status.idle":"2024-12-10T12:33:03.889718Z","shell.execute_reply.started":"2024-12-10T12:33:03.833415Z","shell.execute_reply":"2024-12-10T12:33:03.888905Z"}},"outputs":[{"name":"stdout","text":"Input: torch.Size([4, 512])\nTarget: torch.Size([4, 512])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## GPT High Level Architecture:\n    - Embedding\n    - Positional Encoding\n    - Self Attention Block\n    - Multi Headed Attention\n    - Multi-Layer Perceptron \n\n### Code Implementation adapted from https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py \n","metadata":{}},{"cell_type":"markdown","source":"## Understanding the Embedding Layer in GPT\r\n\r\nIn the GPT model, the **`nn.Embedding` layer** maps **vocabulary indices** (produced by Byte Pair Encoding or other tokenizers) to **dense vector representations** in a continuous space. Let’s break this down step by step to understand how it works, why it’s essential for the GPT architecture, and how it facilitates proper learning.\r\n\r\n---\r\n\r\n### 1. **How Vocab Indices Work with the `nn.Embedding` Layer**\r\n\r\n- After tokenizing the input text with **Byte Pair Encoding (BPE)**, each token is represented as an **integer index** (ranging from `0` to `V-1`, where `V` is the vocabulary size).\r\n- The `nn.Embedding` layer takes these indices and looks up corresponding rows in an **embedding matrix** of shape `(V, d_model)`:\r\n    - `V` = Vocabulary size\r\n    - `d_model` = Dimension of the embedding vectors (hidden size)\r\n\r\nMathematically:\r\n\\[\r\n\\text{eEbedding}(i) = W[i], \\quadE\\text{where } W \\in \\mathbb{R}^{V \\times d_{\\text{model}}} \\text{ is the embedding matrix, and } i \\text{ is the token index}.\r\n\\]\r\n\r\n- Each token index `i` directly selects the `i`-th row (a dense vector of size `d_model`) from the embedding matrix.\r\n\r\n#### Example:\r\nIf the vocabulary has size `10,000` (`V = 10,000`) and `d_model = 768`:\r\n- ThE embedding matrix \\( W \\) has dimensions `(10,000, 768)`.\r\n- A token with index `123` corresponds to the **123rd row** of this matrix, which is a vector of shape `(768,)`.\r\n\r\n---\r\n\r\n### 2. **How Indices Facilitate Proper Learning and Gradient Flow**\r\n\r\nIn PyTorch, the `nn.Embedding` layer is trainable becEuse the embedding matrix \\( W \\) is initialized with random values and updated during training via **backpropagation**.\r\n\r\n- When the input indices pass through the `nn.Embedding` layer, they are used to select the corresponding rows from the embedding matrix. These selected rows act as the \"input\" to the model.\r\n- During **backward propagation**:\r\n    - Gradients are Eomputed only for the rows of \\( W \\) corresponding to the input indices.\r\n    - The rest of the embedding matrix remains unchanged.\r\n\r\nThus, the model learns a **meaningful vector representation** for each token by updating its embedding based on the loss function.\r\n\r\n---\r\n\r\n### 3. **Why is the `nn.Embedding` Layer So Important?**\r\n\r\nThe `nn.Embedding` layer is critical for several reasons:\r\n\r\n1. **Mapping Discrete Indices to Continuous Space**:  \r\n   - Input tokens are discrete (indices), but neural networks operate in continuous space. The `nn.Embedding` layer bridges this gap by mapping indices to dense vectors.\r\n   - These dense vectors encode semantic and syntactic information about words in a high-dimensional space.\r\n\r\n2. **Enabling Representation Learning**:  \r\n   - The embedding vectors are **trainable parameters** that get optimized during training.\r\n   - The model learns embeddings such that semantically similar tokens have similar vector representations (e.g., \"cat\" and \"dog\" may have embeddings close to each other).\r\n\r\n3. **Dimensionality Reduction**:  \r\n   - Instead of using a large one-hot encoding of size `V` (which is sparse and inefficient), embeddings reduce the dimensionality to `d_model` while retaining relevant information.\r\n\r\n4. **Efficient Learning**:  \r\n   - Because gradients are updated only for the indices present in the input, the learning process is efficient and computationally feasible.\r\n\r\n5. **Basis for Contextual Representations**:  \r\n   - In GPT, the (next section)token embeddings are combined with **positional encodings** before passing through the transformer layers.\r\n   - The `nn.Embedding` layer provides the initial \"static\" representation for tokens, which is later refined into **cembedding layer serves as the foundation for building contextual representations in GPT.\r\n","metadata":{}},{"cell_type":"markdown","source":"## Understanding Positional Encoding in GPT\r\n\r\nIn the GPT architecture, **positional encoding** is used to provide information about the position of tokens in a sequence. Since transformers lack the inherent sequential structure of RNNs, positional encodings are added to the input token embeddings to ensure that the model can distinguish the order of tokens.\r\n\r\n---\r\n\r\n### 1. **Why Positional Encoding is Needed**\r\n\r\nTransformers process tokens in parallel without regard to their order. While this parallelism is a major advantage, the model needs a way to understand the **relative or absolute positions** of tokens in a sequence.\r\n\r\n- Token embeddings alone do not contain positional information.\r\n- Positional encoding adds this **position-specific information** to the token embeddings, enabling the model to learn the sequential structure of the input.\r\n\r\n---\r\n\r\n### 2. **How Positional Encoding Works**\r\n\r\nIn GPT, positional encodings are added **element-wise** to the token embeddings. These encodings are typically **learned embeddings** (as opposed to fixed sinusoidal encodings used in some other transformers like BERT or the original Transformer).\r\n\r\nLet’s break this down step by step:\r\n\r\n- Suppose there is a context length, `L` and the embedding dimension is `d_model`.\r\n- GPT maintains a **positional embedding matrix**, `P` of shape `(L, d_model)`.\r\n- Each position `i` in the sequence has a corresponding positional embedding `P[i]` of size `(,d_model)`.\r\n\r\n#### Mathematically:\r\n\r\nLet `E[i]` be the token embedding at position `i` and `P[i]` be the positional embedding for position `i`. The combined input embedding is: \r\n`X[i] = E[i] + P[i]`\r\n\r\n---\r\n\r\n### 3. **How Positional Encoding is Implemented in PyTorch**\r\n\r\nIn GPT, positional encodings are implemented as **learnable embeddings** using `nn.Embedding`:\r\n\r\n- A positional embedding matrix `P` of shape `(L, d_model)` is defined, where:\r\n    - `L` is the maximum allowed sequence length(***THE MODEL'S CONTEXT WINDOW***).\r\n    - `d_model` is the embedding dimension.\r\n- The position indices (0, 1, 2, ..., L-1) are passed to the positional embedding layer, and the resulting embeddings `P[i]` are added to the token embeddings.","metadata":{}},{"cell_type":"markdown","source":"## Understanding the Self-Attention Block in GPT\r\n\r\nThe **self-attention block** is the core computational unit in GPT's transformer architecture. It allows the model to process each token in the sequence while attending to all other tokens to determine their relationships. In GPT, the self-attention is both **autoregressive** and **causal**, meaning each token can only \"attend to\" previous tokens, ensuring proper sequential flow for tasks like language modeling.\r\n\r\n---\r\n\r\n### 1. **Why is the Self-Attention Block So Important?**\r\n\r\nSelf-attention solves a major problem in sequence modeling: **capturing long-range dependencies** between tokens. Unlike RNNs, which process input sequentially and struggle with long-term relationships, self-attention operates in parallel and considers the entire sequence at once.\r\n\r\nKey benefits:\r\n- **Global Context**: Self-attention computes relationships between every token pair, enabling the model to capture global context.\r\n- **Parallel Computation**: Unlike RNNs, the transformer processes all tokens simultaneously, greatly improving efficiency.\r\n- **Dynamic Weighting**: Instead of fixed connections, self-attention dynamically learns which tokens are most relevant to each other.\r\n\r\n---\r\n\r\n### 2. **Key Concepts: Query, Key, and Value Matrices (Q, K, V)**\r\n\r\nThe self-attention mechanism works by projecting input embeddings into three distinct representations:\r\n- **Query (Q)**: Represents the \"current token\" that is trying to find relevant tokens in the sequence.\r\n- **Key (K)**: Represents all tokens in the sequence and is used to determine how relevant they are to the query.\r\n- **Value (V)**: Represents the actual content of the tokens that will be combined based on the attention scores.\r\n\r\n#### How Q, K, V are Computed:\r\nGiven an input embedding matrix `X` of shape `(L, d_model)`, where `L` is the sequence length and `d_model` is the embedding dimension:\r\n- Q, K, V are obtained by applying **learnable linear transformations** (weight matrices) to `X`:\r\nQ = X*W_Q, K = X*W_K, V = X*W_V\r\nWhere:\r\n- `W_Q, W_K, W_V` will each have the shape `(d_model, hidden_dim)`.\r\n- `Q, K, V` will each have the shape `(L, hidden_dim)`.\r\n- `hidden_dim` is another dimension to project the embedding space to another continuous latent space.\r\n\r\n---\r\n\r\n### 3. **Attention Scores and the Attention Mechanism**\r\n\r\nOnce `Q, K, and V` are obtained, the **attention scores** determine how much focus each token should place on others.\r\n\r\n#### Step-by-Step Computation:\r\n\r\n1. **Compute Raw Attention Scores**:\r\n   The attention scores, `alpha`, are computed by taking the dot product of `Q` and `K^T` (transpose of K):\r\n   - Shape of  `QK^T` is `(L, L)`\r\n\r\n2. **Scale the Scores**:\r\n   The raw attention scores, `alpha` are divied by square root of `hidden_dim` to prevent large values (which could destabilize softmax)\r\n   - Denote scaled attention scores, with `alpha_scaled`\r\n\r\n3. **Masking for Causal Attention**:\r\n   In GPT, the self-attention is **autoregressive and causal**. To ensure that each token can only attend to **itself and previous tokens**, a **causal mask** is applied:\r\n   - Tokens at position `i` cannot attend to positions `j > i`, where `0 <= i, j <= L`.\r\n   - This masking sets scores for future positions to `-infinity` so that the softmax outputs zero probabilities for future tokens.\r\n\r\n4. **Apply Softmax**:\r\n   The scaled and masked scores are passed through a softmax function to get the **attention weights. `A`**:\r\n   - Shape of Attention Weights, `A`, is `(L, L)`.\r\n\r\n5. **Weighted Sum of Values**:\r\n   The attention weights, `A` are multiplied with the Value matrix, `V`, to compute the final output, `O`:\r\n   - `O = A * V`\r\n   - Shape of Output, `O`, is `(L, hidden_dim)`.\r\n\r\n---\r\n\r\n### 4. **Why Self-Attention is So Powerful**\r\n\r\nThe self-attention block allows the model to:\r\n1. **Learn Relationships Between All Tokens**: Tokens can dynamically interact with all previous tokens, capturing complex dependencies.\r\n2. **Parallelize Computation**: The dot-product operations on Q, K, and V enable efficient parallel processing.\r\n3. **Focus on Relevant Tokens**: Attention weights allow the model to emphasize important tokens while downweighting irrelevant ones.\r\n\r\nBy applying the causal mask, GPT ensures that the self-attention remains **unidirectional** and suitable for autoregressive tasks like text generation.\r\n","metadata":{}},{"cell_type":"markdown","source":"## Understanding Multi-Head Attention in GPT\r\n\r\nWhile a single self-attention block allows the model to capture relationships between tokens in a sequence, **multi-head attention** extends this idea by allowing the model to focus on different aspects of the input sequence simultaneously. Instead of using a single set of `Q`, `K`, and `V` matrices, multi-head attention computes several attention \"heads\" in parallel. Each head can focus on different parts of the sequence, allowing the model to learn more diverse representations of the input.\r\n\r\n---\r\n\r\n### 1. **Why Multi-Head Attention is Used**\r\n\r\nThe key reason for using multi-head attention is that it allows the model to capture multiple types of relationships between tokens at once. A single attention head may focus on one particular type of interaction or context between tokens, but by using multiple heads, the model can capture **different subspaces of attention**. This helps the model learn richer, more nuanced representations of the input sequence.\r\n\r\nKey benefits of multi-head attention:\r\n- **Captures Multiple Perspectives**: Each attention head focuses on different aspects of the sequence, allowing the model to learn a variety of dependencies (e.g., syntax, semantics, long-range dependencies).\r\n- **Improves Expressiveness**: With multiple heads, the model has more capacity to learn complex relationships in the data.\r\n- **Efficient Computation**: Multi-head attention allows the model to process multiple \"views\" of the data simultaneously, increasing parallelism and computational efficiency.\r\n\r\n---\r\n\r\n### 2. **How Multi-Head Attention Works**\r\n\r\nIn multi-head attention, the self-attention mechanism is applied multiple times in parallel, each with its own set of `W_Q`, `W_K`, and `W_V` matrices. These attention heads are then combined to form the final output. \r\n\r\n#### Step-by-Step Process:\r\n\r\n1. **Linear Projections for Each Head**: \r\n   For `h` attention heads, the input `X` is projected into `h` different sets of `W_Q, W_K, and W_V` matrices. \r\n   - Each set of `W_Q, W_K, and W_V` matrices is of shape `(d_model, hidden_dim // h)`. The 2nd dimension is divided by `h`, the number of heads, because we will add the output from each head together to get back to the original hidden dimension space when we had a single attention head.\r\n\r\n2. **Compute Attention for Each Head**:\r\n   Each attention head, `i`,  computes the attention output independently:\r\n    `O = Attention_Head(X, W_Q_i, W_K_i, W_V_i)`\r\n  \r\n3. **Concatenate the Outputs**:\r\n   The outputs from all heads are concatenated along the last dimension:\r\n   `Multi_Head_O = concat(O_1, O_2, ... O_h)`\r\n   - `Multi_Head_O has shape (L, hidden_dim)`\r\n\r\n\r\n### 3. **Efficient Implementation of Multi-Head Attention in PyTorch**\r\n\r\nTo implement multi-head attention efficiently in PyTorch, we can utilize matrix operations to split a single set of matrices `W_Q, W_K, and W_V` to manage all the heads. This is more efficient because we will perform a single set of matrix multiplication as opposed to a set of matrix multiplications.\r\n\r\n\r\n","metadata":{}},{"cell_type":"markdown","source":"## Understanding the Multi-Layer Perceptron (MLP) in GPT\r\n\r\nThe **Multi-Layer Perceptron (MLP)** in GPT sits at the end of the sequence of multi-head attention blocks and is a critical component for generating output from the transformed representations. It helps to map the high-level, context-aware representations generated by the self-attention layers into the final predictions (such as token probabilities in language modeling tasks).\r\n\r\n---\r\n\r\n### 1. **Why is the Multi-Layer Perceptron Used?**\r\n\r\nThe MLP is essential for two primary reasons:\r\n- **Non-linearity and Transformation**: While the self-attention layers capture complex dependencies between tokens, the MLP introduces non-linearity and helps the model learn complex mappings between the learned representations and the final output space. Without the MLP, the model would be limited to linear transformations and would not be able to learn the rich, non-linear patterns that are required for tasks like language generation.\r\n- **Output Generation**: The MLP maps the processed representations into the model’s output space, such as predicting the next token in a sequence. This allows GPT to make decisions based on the contextualized information learned from the input sequence.","metadata":{}},{"cell_type":"code","source":"VOCAB_SIZE = 50257 #Found from tiktoken's docs. The BPE tokenizer has 50,257 unique tokens\nEMBED_DIM = 512 #Hyperparameter for the dimension of latent space used in attention mechanisms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:33:09.474567Z","iopub.execute_input":"2024-12-10T12:33:09.474904Z","iopub.status.idle":"2024-12-10T12:33:09.478988Z","shell.execute_reply.started":"2024-12-10T12:33:09.474875Z","shell.execute_reply":"2024-12-10T12:33:09.478158Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size, dropout):\n        super().__init__()\n        self.key = nn.Linear(EMBED_DIM, head_size, bias=False)\n        self.query = nn.Linear(EMBED_DIM, head_size, bias=False)\n        self.value = nn.Linear(EMBED_DIM, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(CONTEXT_WINDOW, CONTEXT_WINDOW)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)  \n        q = self.query(x) \n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x) \n        out = wei @ v\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size, dropout):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, dropout) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, EMBED_DIM)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, dropout):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(EMBED_DIM, 4 * EMBED_DIM),\n            nn.ReLU(),\n            nn.Linear(4 * EMBED_DIM, EMBED_DIM),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_head, dropout):\n        super().__init__()\n        head_size = EMBED_DIM // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, dropout)\n        self.ffwd = FeedFoward(dropout)\n        self.ln1 = nn.LayerNorm(EMBED_DIM)\n        self.ln2 = nn.LayerNorm(EMBED_DIM)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self, n_layer: int, n_head: int, dropout: float):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, EMBED_DIM)\n        self.position_embedding_table = nn.Embedding(CONTEXT_WINDOW, EMBED_DIM)\n        self.blocks = nn.Sequential(*[Block(n_head, dropout) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(EMBED_DIM) \n        self.lm_head = nn.Linear(EMBED_DIM, VOCAB_SIZE)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) \n        x = tok_emb + pos_emb\n        x = self.blocks(x) \n        x = self.ln_f(x)\n        logits = self.lm_head(x) \n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -CONTEXT_WINDOW:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:33:10.272379Z","iopub.execute_input":"2024-12-10T12:33:10.272726Z","iopub.status.idle":"2024-12-10T12:33:10.294160Z","shell.execute_reply.started":"2024-12-10T12:33:10.272696Z","shell.execute_reply":"2024-12-10T12:33:10.293303Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = GPTLanguageModel(4, 4, .3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:33:11.038952Z","iopub.execute_input":"2024-12-10T12:33:11.039614Z","iopub.status.idle":"2024-12-10T12:33:12.154682Z","shell.execute_reply.started":"2024-12-10T12:33:11.039578Z","shell.execute_reply":"2024-12-10T12:33:12.153716Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print(device)\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:33:14.014393Z","iopub.execute_input":"2024-12-10T12:33:14.015339Z","iopub.status.idle":"2024-12-10T12:33:14.283322Z","shell.execute_reply.started":"2024-12-10T12:33:14.015290Z","shell.execute_reply":"2024-12-10T12:33:14.282389Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"LR = .0003\nEPOCHS = 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:33:18.256471Z","iopub.execute_input":"2024-12-10T12:33:18.257157Z","iopub.status.idle":"2024-12-10T12:33:18.260728Z","shell.execute_reply.started":"2024-12-10T12:33:18.257125Z","shell.execute_reply":"2024-12-10T12:33:18.259793Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n\nfor iter in range(EPOCHS):\n    epoch_loss = 0\n    for input_text, target_text in dataloader:\n        input_text, target_text = input_text.to(device), target_text.to(device)\n        \n    # evaluate the loss\n    logits, loss = model(input_text, target_text)\n    epoch_loss += loss\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    print(f\"EPOCH {iter} LOSS: {epoch_loss}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:33:19.062427Z","iopub.execute_input":"2024-12-10T12:33:19.063326Z","iopub.status.idle":"2024-12-10T12:34:02.747224Z","shell.execute_reply.started":"2024-12-10T12:33:19.063277Z","shell.execute_reply":"2024-12-10T12:34:02.746459Z"}},"outputs":[{"name":"stdout","text":"EPOCH 0 LOSS: 10.93112564086914\nEPOCH 1 LOSS: 10.000994682312012\nEPOCH 2 LOSS: 9.641437530517578\nEPOCH 3 LOSS: 9.373924255371094\nEPOCH 4 LOSS: 9.183527946472168\nEPOCH 5 LOSS: 8.938088417053223\nEPOCH 6 LOSS: 8.698592185974121\nEPOCH 7 LOSS: 8.507721900939941\nEPOCH 8 LOSS: 8.277900695800781\nEPOCH 9 LOSS: 7.961567401885986\nEPOCH 10 LOSS: 7.770715713500977\nEPOCH 11 LOSS: 7.54622220993042\nEPOCH 12 LOSS: 7.37234354019165\nEPOCH 13 LOSS: 7.11744499206543\nEPOCH 14 LOSS: 6.867424488067627\nEPOCH 15 LOSS: 6.679500579833984\nEPOCH 16 LOSS: 6.509734630584717\nEPOCH 17 LOSS: 6.226180553436279\nEPOCH 18 LOSS: 5.9952921867370605\nEPOCH 19 LOSS: 5.9894537925720215\nEPOCH 20 LOSS: 5.7114996910095215\nEPOCH 21 LOSS: 5.5000152587890625\nEPOCH 22 LOSS: 5.403318881988525\nEPOCH 23 LOSS: 5.1641669273376465\nEPOCH 24 LOSS: 5.266387462615967\nEPOCH 25 LOSS: 5.0275492668151855\nEPOCH 26 LOSS: 4.850244045257568\nEPOCH 27 LOSS: 4.701484680175781\nEPOCH 28 LOSS: 4.434895038604736\nEPOCH 29 LOSS: 4.493546485900879\nEPOCH 30 LOSS: 4.215881824493408\nEPOCH 31 LOSS: 4.138134956359863\nEPOCH 32 LOSS: 4.308611869812012\nEPOCH 33 LOSS: 3.8695499897003174\nEPOCH 34 LOSS: 3.862483024597168\nEPOCH 35 LOSS: 3.591878652572632\nEPOCH 36 LOSS: 3.6283435821533203\nEPOCH 37 LOSS: 3.6326329708099365\nEPOCH 38 LOSS: 3.569021224975586\nEPOCH 39 LOSS: 3.766515016555786\nEPOCH 40 LOSS: 3.36328387260437\nEPOCH 41 LOSS: 3.3458614349365234\nEPOCH 42 LOSS: 3.154546022415161\nEPOCH 43 LOSS: 2.997636079788208\nEPOCH 44 LOSS: 2.95395565032959\nEPOCH 45 LOSS: 2.8201379776000977\nEPOCH 46 LOSS: 3.0508947372436523\nEPOCH 47 LOSS: 2.651258945465088\nEPOCH 48 LOSS: 2.6491591930389404\nEPOCH 49 LOSS: 2.462862491607666\nEPOCH 50 LOSS: 2.690638542175293\nEPOCH 51 LOSS: 2.9598658084869385\nEPOCH 52 LOSS: 2.3953857421875\nEPOCH 53 LOSS: 2.2259743213653564\nEPOCH 54 LOSS: 2.3187479972839355\nEPOCH 55 LOSS: 2.4542646408081055\nEPOCH 56 LOSS: 2.261812925338745\nEPOCH 57 LOSS: 2.559051513671875\nEPOCH 58 LOSS: 2.165087938308716\nEPOCH 59 LOSS: 2.6796858310699463\nEPOCH 60 LOSS: 1.9082980155944824\nEPOCH 61 LOSS: 1.9574737548828125\nEPOCH 62 LOSS: 1.719119668006897\nEPOCH 63 LOSS: 1.890138030052185\nEPOCH 64 LOSS: 1.747103214263916\nEPOCH 65 LOSS: 1.9549766778945923\nEPOCH 66 LOSS: 1.689534306526184\nEPOCH 67 LOSS: 1.8021659851074219\nEPOCH 68 LOSS: 1.5878087282180786\nEPOCH 69 LOSS: 1.597799301147461\nEPOCH 70 LOSS: 1.4989237785339355\nEPOCH 71 LOSS: 1.6330798864364624\nEPOCH 72 LOSS: 1.6318079233169556\nEPOCH 73 LOSS: 1.4731389284133911\nEPOCH 74 LOSS: 1.375118374824524\nEPOCH 75 LOSS: 1.436008334159851\nEPOCH 76 LOSS: 1.4940286874771118\nEPOCH 77 LOSS: 1.4839543104171753\nEPOCH 78 LOSS: 1.3427077531814575\nEPOCH 79 LOSS: 1.2848294973373413\nEPOCH 80 LOSS: 1.2930270433425903\nEPOCH 81 LOSS: 1.2125729322433472\nEPOCH 82 LOSS: 1.2689993381500244\nEPOCH 83 LOSS: 1.3441554307937622\nEPOCH 84 LOSS: 1.1821295022964478\nEPOCH 85 LOSS: 1.3164016008377075\nEPOCH 86 LOSS: 1.2864652872085571\nEPOCH 87 LOSS: 1.0807034969329834\nEPOCH 88 LOSS: 1.39558744430542\nEPOCH 89 LOSS: 1.2889574766159058\nEPOCH 90 LOSS: 1.348366141319275\nEPOCH 91 LOSS: 1.2658659219741821\nEPOCH 92 LOSS: 1.0753649473190308\nEPOCH 93 LOSS: 1.1125518083572388\nEPOCH 94 LOSS: 1.2437444925308228\nEPOCH 95 LOSS: 1.2264347076416016\nEPOCH 96 LOSS: 0.9472646713256836\nEPOCH 97 LOSS: 0.9915127158164978\nEPOCH 98 LOSS: 1.1435425281524658\nEPOCH 99 LOSS: 1.2829927206039429\nEPOCH 100 LOSS: 1.1054636240005493\nEPOCH 101 LOSS: 1.0210989713668823\nEPOCH 102 LOSS: 1.0166800022125244\nEPOCH 103 LOSS: 1.0016188621520996\nEPOCH 104 LOSS: 1.0416420698165894\nEPOCH 105 LOSS: 1.1097177267074585\nEPOCH 106 LOSS: 0.9445090293884277\nEPOCH 107 LOSS: 0.9864950776100159\nEPOCH 108 LOSS: 0.9604135155677795\nEPOCH 109 LOSS: 1.044493556022644\nEPOCH 110 LOSS: 0.872072696685791\nEPOCH 111 LOSS: 0.7960996031761169\nEPOCH 112 LOSS: 0.8478997349739075\nEPOCH 113 LOSS: 0.9325624108314514\nEPOCH 114 LOSS: 0.7858137488365173\nEPOCH 115 LOSS: 0.8987627625465393\nEPOCH 116 LOSS: 0.8100747466087341\nEPOCH 117 LOSS: 0.8418381810188293\nEPOCH 118 LOSS: 1.0588797330856323\nEPOCH 119 LOSS: 0.9130701422691345\nEPOCH 120 LOSS: 1.2823737859725952\nEPOCH 121 LOSS: 0.7144282460212708\nEPOCH 122 LOSS: 0.7848168015480042\nEPOCH 123 LOSS: 0.8944570422172546\nEPOCH 124 LOSS: 1.0420887470245361\nEPOCH 125 LOSS: 0.7486355900764465\nEPOCH 126 LOSS: 0.7001474499702454\nEPOCH 127 LOSS: 0.7089311480522156\nEPOCH 128 LOSS: 1.015668272972107\nEPOCH 129 LOSS: 0.6969484686851501\nEPOCH 130 LOSS: 0.9224838614463806\nEPOCH 131 LOSS: 0.6907166838645935\nEPOCH 132 LOSS: 0.8600820899009705\nEPOCH 133 LOSS: 0.7045637965202332\nEPOCH 134 LOSS: 0.6441532969474792\nEPOCH 135 LOSS: 0.7295951247215271\nEPOCH 136 LOSS: 0.672295093536377\nEPOCH 137 LOSS: 0.5845885872840881\nEPOCH 138 LOSS: 0.6249012351036072\nEPOCH 139 LOSS: 0.748820960521698\nEPOCH 140 LOSS: 0.6172394156455994\nEPOCH 141 LOSS: 0.6033066511154175\nEPOCH 142 LOSS: 0.6751782894134521\nEPOCH 143 LOSS: 0.5943532586097717\nEPOCH 144 LOSS: 0.6430392265319824\nEPOCH 145 LOSS: 0.6042941212654114\nEPOCH 146 LOSS: 0.6109055876731873\nEPOCH 147 LOSS: 0.674079179763794\nEPOCH 148 LOSS: 0.7685606479644775\nEPOCH 149 LOSS: 0.5357539653778076\nEPOCH 150 LOSS: 0.5939204692840576\nEPOCH 151 LOSS: 0.496142715215683\nEPOCH 152 LOSS: 0.5480203032493591\nEPOCH 153 LOSS: 0.564075767993927\nEPOCH 154 LOSS: 0.45334064960479736\nEPOCH 155 LOSS: 0.5590801239013672\nEPOCH 156 LOSS: 0.5904514193534851\nEPOCH 157 LOSS: 0.5137030482292175\nEPOCH 158 LOSS: 0.49002301692962646\nEPOCH 159 LOSS: 0.6055221557617188\nEPOCH 160 LOSS: 0.5289784073829651\nEPOCH 161 LOSS: 0.655663788318634\nEPOCH 162 LOSS: 0.540409505367279\nEPOCH 163 LOSS: 0.5186256170272827\nEPOCH 164 LOSS: 0.5034026503562927\nEPOCH 165 LOSS: 0.549804151058197\nEPOCH 166 LOSS: 0.4958193302154541\nEPOCH 167 LOSS: 0.6254176497459412\nEPOCH 168 LOSS: 0.6039131283760071\nEPOCH 169 LOSS: 0.6940972805023193\nEPOCH 170 LOSS: 0.6056699156761169\nEPOCH 171 LOSS: 0.5061582922935486\nEPOCH 172 LOSS: 0.3786051273345947\nEPOCH 173 LOSS: 0.5112525224685669\nEPOCH 174 LOSS: 0.5635469555854797\nEPOCH 175 LOSS: 0.6291217803955078\nEPOCH 176 LOSS: 0.4872141182422638\nEPOCH 177 LOSS: 0.46994277834892273\nEPOCH 178 LOSS: 0.4879564344882965\nEPOCH 179 LOSS: 0.5869315266609192\nEPOCH 180 LOSS: 0.4924789369106293\nEPOCH 181 LOSS: 0.42022788524627686\nEPOCH 182 LOSS: 0.564538300037384\nEPOCH 183 LOSS: 0.5147219896316528\nEPOCH 184 LOSS: 0.40085306763648987\nEPOCH 185 LOSS: 0.39740195870399475\nEPOCH 186 LOSS: 0.3766460716724396\nEPOCH 187 LOSS: 0.5145673155784607\nEPOCH 188 LOSS: 0.4180125892162323\nEPOCH 189 LOSS: 0.3973526656627655\nEPOCH 190 LOSS: 0.40058377385139465\nEPOCH 191 LOSS: 0.30610331892967224\nEPOCH 192 LOSS: 0.44614720344543457\nEPOCH 193 LOSS: 0.47083890438079834\nEPOCH 194 LOSS: 0.4287790358066559\nEPOCH 195 LOSS: 0.4546203911304474\nEPOCH 196 LOSS: 0.33625057339668274\nEPOCH 197 LOSS: 0.37762096524238586\nEPOCH 198 LOSS: 0.4261495769023895\nEPOCH 199 LOSS: 0.4043984115123749\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(bpe_tokenizer.decode(model.generate(context, max_new_tokens=500)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:34:06.403819Z","iopub.execute_input":"2024-12-10T12:34:06.404291Z","iopub.status.idle":"2024-12-10T12:34:11.736405Z","shell.execute_reply.started":"2024-12-10T12:34:06.404259Z","shell.execute_reply":"2024-12-10T12:34:11.735566Z"}},"outputs":[{"name":"stdout","text":"!IVustTI sugar cemeteryS sound ResY OF ACHILLES\n THE HIS SPEARATH OF H present18III.\n VESSELS\n JUPITER\n THE CHIDING HISOS CASE\n THE MEETING OF HECTOR ANDROMACHE\n IRIS\nQUÆCI� freshOMER\nprisingTAR\n HECTOR AND AND DEATH OF PARIS\n HECTOR ANDANCE OF HECTOR AND AJAX SES CAPTUNE ORDERING PARIS\n GREEKS\n THE HOURS TAKING THE HERALDS\n DIOMED AND AJAX SEPARATED BY THE HORSES FROM JUNO’S CAR\n PLUTO\n PLUTO’ FUNERAL OF ACHILLES\n PLUTO\n THE EMBASSY TO ACHILLES\n GREEK GALLEY\n GREEK GALLEY\n GREEK GREEK GREEK GREEK GREEKCont\n DIOMED AND POLLUX\n DI nar AND ULYSSES RETUR ResponsibilityASSY TO RHESUS\n PROSERPINE\n ACHILLESUS\n DIOMED AND ULYANCE OF RHESUS\n DIOMED AND ULYSSESoras RISING FROM THE DESCENT OF SLEEPTUNE RISING THE SPOILS OF DISCORD\n IACCHILLESUS\n DIANA\n AJAX SEPARATED BY THE SPOILS OF DISCULESUS\n SLEEP ESCAPING THE SEA\n HERC NovemberED AND ULYSSES\n BACCHUS\n SLEEPTAR\n GALLEY\n BACCHUS\n AJAX DEFENDING THE DESCENT OF exploitationUS\n CASTOR AND POLLUX\n ÆSCULAPIUS\n JUD centuryEDON TO BODY OF SLEEP ESCAPING THE INF562 ScientK SHIPS\n DIANA\n DIANA\n FIGHT FOR THE WRATH CONVEYING THE BODY OF SARPEDON TO DESCEND IsaARPEDON TO DESCEND INTO THE SUN TO OB JUNO COMMANDING THE BODY OF PATROCLUS\n THETIS ORDERING THE SUN TOoused WITH THE NEREIDS TO moons\n FIGHT FOR THE INFANT VULCAN AND EURINGS\n HERCORD\n TRIPOD\n VULCAN\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Issue with Pretraining on the Iliad\r\n\r\nI trained a G-2 model on the plain text of *The Iliad* by Homer. After pretraining, the output of 500 tokens was nonsensical and repetitive, showing a lack of meaningful context or coherence. The output consisteofnd random references to characters and events in the text, indicating that the model had simply memorized blocks o*T the Ili*ad rather than learning useful patterns of language.\r\n\r\n### Why This Happened\r\nThe core issue is that *The Iliad* is a relatively small text, with a limited vocabulary and context. The small corpus means that the model cannot generalize well and has mostly memorized specific passages. Additionally, the limited vocabulary prevents the model from learning the complex emergent behaviors seen in large language models (LLMs) that allow for creativity, generalization, and coherence.\r\n\r\n### Solution: Fine-tuning on Pretrained GPT-2 Weights\r\nTo overcome this, I plan to use GPT-2 weights from HuggingFace's Transformers library. The GPT-2 model has already been pretrained on a vast text corpus, so it has learned general language patterns and emergent behaviors. By fine-tuning the model on *The Iliad*, I can quickly adapt the model to be an expert in the text of *The Iliad*, while retaining the benefits of large-scale pretraining on diverse data. This approach strikes a balance between efficient fine-tuning and leveraging the powerful capabilities of a pretrained model.\r\n","metadata":{}},{"cell_type":"markdown","source":"## Still Can't Just Finetune on Plaintext 'The Iliad'\n - If I finetune gpt-2 on *The Iliad*, the finetuned model would just know blocks of text from the book. That does not provide value to any user.\n - Building a chatbot requires finetuning a foundation model, like GPT-2, on a question answer dataset.\n - How can the plaintext book be used to build a domain specific Q&A dataset? What if I split the plaintext book into chunks, ask a LLM to generate a question on the chunks, then feed the question, passage pairs to another LLM and ask for the answer. This pipeline could yield a massive Q&A dataset about *The Iliad*.\n - After building this Q&A dataset, I will finetune GPT-2 on the dataset and the result will be an extremely useful Iliad-GPT chatbot!","metadata":{}},{"cell_type":"code","source":"def split_text_into_chunks(text, chunk_size=700, overlap=50):\n    chunks = []\n    start = 0\n\n    while start < len(text):\n        end = start + chunk_size\n        chunk = text[start:end]\n        \n        # Ensure the chunk does not start or end mid-word\n        if end < len(text):\n            last_space = chunk.rfind(' ')\n            if last_space != -1:\n                chunk = chunk[:last_space]\n                end = start + len(chunk)\n                \n        if len(chunk) >= chunk_size // 2:\n            chunks.append(chunk.strip())\n            \n        start = end - overlap\n    \n    return chunks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T02:51:14.745465Z","iopub.execute_input":"2024-12-10T02:51:14.746178Z","iopub.status.idle":"2024-12-10T02:51:14.751590Z","shell.execute_reply.started":"2024-12-10T02:51:14.746144Z","shell.execute_reply":"2024-12-10T02:51:14.750691Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"chunks = split_text_into_chunks(iliad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T02:51:15.765460Z","iopub.execute_input":"2024-12-10T02:51:15.766104Z","iopub.status.idle":"2024-12-10T02:51:15.773381Z","shell.execute_reply.started":"2024-12-10T02:51:15.766070Z","shell.execute_reply":"2024-12-10T02:51:15.772342Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"print(len(chunks))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T02:51:16.537224Z","iopub.execute_input":"2024-12-10T02:51:16.537552Z","iopub.status.idle":"2024-12-10T02:51:16.541966Z","shell.execute_reply.started":"2024-12-10T02:51:16.537527Z","shell.execute_reply":"2024-12-10T02:51:16.541181Z"}},"outputs":[{"name":"stdout","text":"1700\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"#Remove first several chunks as they just provide meta data about the actual text\nchunks = chunks[10:]\nprint(len(chunks))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T02:51:17.701369Z","iopub.execute_input":"2024-12-10T02:51:17.701717Z","iopub.status.idle":"2024-12-10T02:51:17.706663Z","shell.execute_reply.started":"2024-12-10T02:51:17.701686Z","shell.execute_reply":"2024-12-10T02:51:17.705764Z"}},"outputs":[{"name":"stdout","text":"1690\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"print(chunks[711])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T02:51:19.833298Z","iopub.execute_input":"2024-12-10T02:51:19.833654Z","iopub.status.idle":"2024-12-10T02:51:19.838657Z","shell.execute_reply.started":"2024-12-10T02:51:19.833619Z","shell.execute_reply":"2024-12-10T02:51:19.837718Z"}},"outputs":[{"name":"stdout","text":"ce a traitor, thou betray’st no more.”\n\nSternly he spoke, and as the wretch prepared\nWith humble blandishment to stroke his beard,\nLike lightning swift the wrathful falchion flew,\nDivides the neck, and cuts the nerves in two;\nOne instant snatch’d his trembling soul to hell,\nThe head, yet speaking, mutter’d as it fell.\nThe furry helmet from his brow they tear,\nThe wolf’s grey hide, the unbended bow and spear;\nThese great Ulysses lifting to the skies,\nTo favouring Pallas dedicates the prize:\n\n“Great queen of arms, receive this hostile spoil,\nAnd let the Thracian steeds reward our toil;\nThee, first of all the heavenly host, we praise;\nO speed our labours, and direct our ways!”\nThis said, the\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"BATCH_SIZE = 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T02:51:21.970466Z","iopub.execute_input":"2024-12-10T02:51:21.971099Z","iopub.status.idle":"2024-12-10T02:51:21.974918Z","shell.execute_reply.started":"2024-12-10T02:51:21.971062Z","shell.execute_reply":"2024-12-10T02:51:21.973978Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"## Q&A LLMs.\n    - The Question Generation model is a finetuned version of the t5 foundation model. This LLM is specialized in generating questions about a context.\n    - The Answer Question model is t5 from google. The answering is done using Transformers' text2text generation pipeline as well.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, T5ForConditionalGeneration\n\nqg_model_name = \"ThomasSimonini/t5-end2end-question-generation\"\nqg_tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\nqg_model = T5ForConditionalGeneration.from_pretrained(qg_model_name).to(device)\n\nquestion_mark_id = qg_tokenizer.encode(\"?\", add_special_tokens=False)[-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:08:15.391806Z","iopub.execute_input":"2024-12-10T03:08:15.392187Z","iopub.status.idle":"2024-12-10T03:08:17.281664Z","shell.execute_reply.started":"2024-12-10T03:08:15.392156Z","shell.execute_reply":"2024-12-10T03:08:17.280643Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"from transformers import pipeline\n\nqa_model_name = \"google/flan-t5-large\"\nqa_pipeline_device = 0 if torch.cuda.is_available() else -1\nqa_pipeline = pipeline(\"text2text-generation\", model=qa_model_name, tokenizer=qa_model_name, device=qa_pipeline_device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:09:39.992775Z","iopub.execute_input":"2024-12-10T03:09:39.993649Z","iopub.status.idle":"2024-12-10T03:09:42.420695Z","shell.execute_reply.started":"2024-12-10T03:09:39.993615Z","shell.execute_reply":"2024-12-10T03:09:42.419951Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"def generate_questions(passages, max_input_length=1000, max_new_tokens=50):\n    questions = []\n\n    for i in range(0, len(passages), BATCH_SIZE):\n        batch = passages[i:i+BATCH_SIZE]\n        \n        # Create input prompts for T5\n        prompts = [f\"Here is a passage from The Iliad by Homer': {passage[:max_input_length]}\" for passage in batch]\n        \n        # Tokenize prompts\n        inputs = qg_tokenizer(prompts, return_tensors=\"pt\", truncation=True, padding=True, max_length=1024).to(device)\n\n        # Generate questions\n        with torch.no_grad():\n            outputs = qg_model.generate(\n                inputs.input_ids,\n                attention_mask=inputs.attention_mask,\n                max_new_tokens=max_new_tokens,\n                eos_token_id=question_mark_id, \n                pad_token_id=qg_tokenizer.pad_token_id,\n                no_repeat_ngram_size=2\n            )\n\n        for j in range(outputs.size(0)):\n            question = qg_tokenizer.decode(outputs[j], skip_special_tokens=True)\n            questions.append(question)\n\n    return questions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:09:43.952713Z","iopub.execute_input":"2024-12-10T03:09:43.953061Z","iopub.status.idle":"2024-12-10T03:09:43.959572Z","shell.execute_reply.started":"2024-12-10T03:09:43.953034Z","shell.execute_reply":"2024-12-10T03:09:43.958494Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"def generate_answers(passages, questions):\n    assert len(passages) == len(questions), \"Passages and questions must have the same length\"\n    answers = []\n\n    for passage, question in zip(passages, questions):\n        prompt = f\"Answer the question about this passage of The Iliad: PASSAGE:{passage} QUESTION:{question} ANSWER:\"\n        try:\n            result = qa_pipeline(prompt, max_length=100, num_return_sequences=1, do_sample=False, truncation=True)\n            answer = result[0]['generated_text'].split(\"ANSWER:\")[-1].strip()\n        except Exception as e:\n            answer = \"Unable to generate an answer.\"\n\n        answers.append(answer)\n\n    return answers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:12:48.739260Z","iopub.execute_input":"2024-12-10T03:12:48.739621Z","iopub.status.idle":"2024-12-10T03:12:48.745429Z","shell.execute_reply.started":"2024-12-10T03:12:48.739591Z","shell.execute_reply":"2024-12-10T03:12:48.744489Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"qa_pairs = []\n\nbatch_size = 8\nfor i in range(0, len(chunks), batch_size):\n    batch_chunks = chunks[i:i+batch_size]\n\n    questions = generate_questions(batch_chunks)\n    answers = generate_answers(batch_chunks, questions)\n    \n    for question, answer in zip(questions, answers):\n        qa_pairs.append({\"question\": question, \"answer\": answer})\n    \n    if i % 80 == 0:\n        print(f\"Chunks complete: {i}/{len(chunks)}\")\n        print(f\"Q:\\n {questions[-1]}\\nA:\\n {answers[-1]}\\n{'-'*40}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:12:50.207436Z","iopub.execute_input":"2024-12-10T03:12:50.207749Z","iopub.status.idle":"2024-12-10T03:21:48.514095Z","shell.execute_reply.started":"2024-12-10T03:12:50.207722Z","shell.execute_reply":"2024-12-10T03:21:48.513368Z"}},"outputs":[{"name":"stdout","text":"Chunks complete: 0/1690\nQ:\n What did Homer say to the Colophomans?\nA:\n the inhabitants showed the place where he used to sit when giving a recitation of his verses, and they greatly honoured the spot\n----------------------------------------\nChunks complete: 80/1690\nQ:\n What is the title of the passage from The Iliad by Homer?\nA:\n The Iliad\n----------------------------------------\nChunks complete: 160/1690\nQ:\n Who was one of the first to favour me?\nA:\n The Earl of Halifax\n----------------------------------------\nChunks complete: 240/1690\nQ:\n What is the name of the passage from The Iliad by Homer?\nA:\n all the thronging train\n----------------------------------------\nChunks complete: 320/1690\nQ:\n What is the name of the passage from The Iliad by Homer?\nA:\n Troy possess her fertile fields in peace\n----------------------------------------\nChunks complete: 400/1690\nQ:\n What is the title of the passage from The Iliad by Homer?\nA:\n to the navy borne\n----------------------------------------\nChunks complete: 480/1690\nQ:\n Whose mandate showed'd?\nA:\n his monarch’s\n----------------------------------------\nChunks complete: 560/1690\nQ:\n What is the passage from The Iliad by Homer's?\nA:\n ament their fate\n----------------------------------------\nChunks complete: 640/1690\nQ:\n Who did Hector keep the verge of?\nA:\n Troy\n----------------------------------------\nChunks complete: 720/1690\nQ:\n What was the chief's gift?\nA:\n Ten rows of azure steel\n----------------------------------------\nChunks complete: 800/1690\nQ:\n What is the passage from The Iliad by Homer's?\nA:\n their fleet their last efforts they try\n----------------------------------------\nChunks complete: 880/1690\nQ:\n What is the title of the passage from The Iliad by Homer?\nA:\n Trojan arrow\n----------------------------------------\nChunks complete: 960/1690\nQ:\n Who did Jove engage in a rage?\nA:\n Ilion and Greece\n----------------------------------------\nChunks complete: 1040/1690\nQ:\n Who received the Spartan lance?\nA:\n Amphidus\n----------------------------------------\nChunks complete: 1120/1690\nQ:\n What did the mangled body bathe in?\nA:\n sweat and blood\n----------------------------------------\nChunks complete: 1200/1690\nQ:\n Who is the hero of the Iliad by Homer?\nA:\n Patroclus\n----------------------------------------\nChunks complete: 1280/1690\nQ:\n What is the passage from The Iliad by Homer?\nA:\n Trojan pale with fears Approach’d,\n----------------------------------------\nChunks complete: 1360/1690\nQ:\n What is Troy?\nA:\n The dire destroyer to our arm\n----------------------------------------\nChunks complete: 1440/1690\nQ:\n What is the name of the great Tydeus' son?\nA:\n Ajax Telamon\n----------------------------------------\nChunks complete: 1520/1690\nQ:\n Who was taken soon after the death of Hector?\nA:\n Troy\n----------------------------------------\nChunks complete: 1600/1690\nQ:\n Who wrote the Iliad by Homer?\nA:\n Cyclic poets\n----------------------------------------\nChunks complete: 1680/1690\nQ:\n Helen laments Hector and hints at her own invidious and unprotected situation in Troy. Helen's speech in which she lamentes Hektor is almost the sweetest passage in the poem. Helen is\nA:\n a genuine lady\n----------------------------------------\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"import json\n\nwith open(\"synthetic_qa_dataset.json\", 'w') as json_file:\n    json.dump({\"dataset\":qa_pairs}, json_file, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T03:23:43.847320Z","iopub.execute_input":"2024-12-10T03:23:43.847666Z","iopub.status.idle":"2024-12-10T03:23:43.862962Z","shell.execute_reply.started":"2024-12-10T03:23:43.847636Z","shell.execute_reply":"2024-12-10T03:23:43.862247Z"}},"outputs":[],"execution_count":103},{"cell_type":"code","source":"import json \n\nwith open(\"/kaggle/input/iliad-gutenberg/synthetic_qa_dataset.json\", \"rb\") as qa_file:\n    text = qa_file.read()\n    qa_pairs = json.loads(text)[\"dataset\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T05:28:44.431888Z","iopub.execute_input":"2024-12-10T05:28:44.432276Z","iopub.status.idle":"2024-12-10T05:28:44.443745Z","shell.execute_reply.started":"2024-12-10T05:28:44.432227Z","shell.execute_reply":"2024-12-10T05:28:44.443033Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Pull GPT-2 Foundation model from HuggingFace","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T05:29:02.162828Z","iopub.execute_input":"2024-12-10T05:29:02.163223Z","iopub.status.idle":"2024-12-10T05:29:04.056847Z","shell.execute_reply.started":"2024-12-10T05:29:02.163191Z","shell.execute_reply":"2024-12-10T05:29:04.056059Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model_name = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side=\"left\")\ntokenizer.pad_token = tokenizer.eos_token\nfoundation_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n\nfoundation_model.eval()\n\n# Generate text using the model (inference)\ninput_text = \"In the land of the Greeks, Achilles stood tall\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n\n# Ensure that no gradient calculation is performed during inference\nwith torch.no_grad():\n    outputs = foundation_model.generate(\n        inputs[\"input_ids\"], \n        max_length=100,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2,\n        top_k=50,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n# Decode the generated token IDs back into text\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Generated Text: \", generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T05:29:35.165174Z","iopub.execute_input":"2024-12-10T05:29:35.165561Z","iopub.status.idle":"2024-12-10T05:29:36.830470Z","shell.execute_reply.started":"2024-12-10T05:29:35.165528Z","shell.execute_reply":"2024-12-10T05:29:36.829546Z"}},"outputs":[{"name":"stdout","text":"Generated Text:  In the land of the Greeks, Achilles stood tall and could speak; and what he found there was the strength and the beauty of life that gave his countrymen strength. So, too, in the northern part of Egypt they held to their sacred law by the law of Moses, and all their people by that old law. This was called the Gath.\n\nAnd so the Egyptians and their forefathers went back to a place called Phrygia and after a while went with the\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Finetuning GPT-2\n    - Use a train and validation dataset to track progress\n    - Note that attention mask is used to prevent model from attending to empty space tokens used to allow batch operations.\n    - Only training for a few epochs to prevent model from overfitting to this small Q&A dataset and to prevent loss of previous knowledge","metadata":{}},{"cell_type":"code","source":"class QADataset(Dataset):\n    def __init__(self, inputs):\n        self.inputs = inputs\n        \n    def __len__(self):\n        return self.inputs[\"input_ids\"].shape[0]\n\n    def __getitem__(self, idx):\n        ids = self.inputs[\"input_ids\"][idx].unsqueeze(0)\n        attention_mask = self.inputs[\"attention_mask\"][idx].unsqueeze(0)\n\n        return ids, attention_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T05:48:21.263040Z","iopub.execute_input":"2024-12-10T05:48:21.263393Z","iopub.status.idle":"2024-12-10T05:48:21.269580Z","shell.execute_reply.started":"2024-12-10T05:48:21.263362Z","shell.execute_reply":"2024-12-10T05:48:21.268414Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_data = []\nfor qa in qa_pairs:\n    prompt = f\"QUESTION: {qa['question']} ANSWER: {qa['answer']}\"\n    train_data.append(prompt)\n\ntrain_qa, val_qa = train_test_split(qa_pairs, test_size=0.1, random_state=11)\n\ndef create_prompts(qa_data):\n    return [f\"QUESTION: {qa['question']} ANSWER: {qa['answer']}\" for qa in qa_data]\n\ntrain_prompts = create_prompts(train_qa)\nval_prompts = create_prompts(val_qa)\n\ntrain_inputs = tokenizer(train_prompts, padding=True, truncation=True, return_tensors=\"pt\", max_length=1024)\nval_inputs = tokenizer(val_prompts, padding=True, truncation=True, return_tensors=\"pt\", max_length=1024)\n\ntrain_dataset = QADataset(train_inputs)\nval_dataset = QADataset(val_inputs)\n\nprint(len(train_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T05:48:21.660418Z","iopub.execute_input":"2024-12-10T05:48:21.661266Z","iopub.status.idle":"2024-12-10T05:48:21.941505Z","shell.execute_reply.started":"2024-12-10T05:48:21.661231Z","shell.execute_reply":"2024-12-10T05:48:21.940463Z"}},"outputs":[{"name":"stdout","text":"1521\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(foundation_model.parameters(), lr=5e-5)\n\nFT_EPOCHS = 3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T05:48:28.597656Z","iopub.execute_input":"2024-12-10T05:48:28.598033Z","iopub.status.idle":"2024-12-10T05:48:28.604811Z","shell.execute_reply.started":"2024-12-10T05:48:28.598006Z","shell.execute_reply":"2024-12-10T05:48:28.603756Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T05:48:29.527415Z","iopub.execute_input":"2024-12-10T05:48:29.528300Z","iopub.status.idle":"2024-12-10T05:48:29.532974Z","shell.execute_reply.started":"2024-12-10T05:48:29.528258Z","shell.execute_reply":"2024-12-10T05:48:29.531941Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"from tqdm import tqdm\n\nfor epoch in range(FT_EPOCHS):\n    foundation_model.train()\n    epoch_loss = 0\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n        input_ids, attention_mask = batch\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n\n        optimizer.zero_grad()\n\n        loss = foundation_model(input_ids, attention_mask=attention_mask, labels=input_ids).loss\n        \n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        \n    eval_loss = 0\n    foundation_model.eval()\n    for batch in val_dataloader:\n        input_ids, attention_mask = batch\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n\n        outputs = foundation_model(input_ids, attention_mask=attention_mask, labels=input_ids)\n        eval_loss += outputs.loss.item()\n\n    print(f\"Epoch {epoch+1}  |  Loss: {epoch_loss / len(train_dataloader)}   |  Validation Loss: {eval_loss / len(val_dataloader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T05:49:12.265657Z","iopub.execute_input":"2024-12-10T05:49:12.266535Z","iopub.status.idle":"2024-12-10T05:50:29.271439Z","shell.execute_reply.started":"2024-12-10T05:49:12.266481Z","shell.execute_reply":"2024-12-10T05:50:29.270199Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 191/191 [00:24<00:00,  7.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1  |  Loss: 0.6799877672139263   |  Validation Loss: 0.5888134674592451\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 191/191 [00:24<00:00,  7.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2  |  Loss: 0.560515381434825   |  Validation Loss: 0.5630648230964487\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 191/191 [00:24<00:00,  7.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3  |  Loss: 0.5037320243750567   |  Validation Loss: 0.5529023788192056\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"foundation_model.save_pretrained(\"./gpt2-finetuned\")\ntokenizer.save_pretrained(\"./gpt2-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T05:51:49.610653Z","iopub.execute_input":"2024-12-10T05:51:49.611323Z","iopub.status.idle":"2024-12-10T05:51:51.220231Z","shell.execute_reply.started":"2024-12-10T05:51:49.611276Z","shell.execute_reply":"2024-12-10T05:51:51.219027Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"('./gpt2-finetuned/tokenizer_config.json',\n './gpt2-finetuned/special_tokens_map.json',\n './gpt2-finetuned/vocab.json',\n './gpt2-finetuned/merges.txt',\n './gpt2-finetuned/added_tokens.json')"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"test_sentences = [\"QUESTION: What is the Iliad about? ANSWER:\", \n                  \"QUESTION: Who are the characters in the Iliad? ANSWER:\", \n                  \"QUESTION:Who is Achilles? ANSWER:\", \n                  \"QUESTION:Who is Achilles' wife? ANSWER:\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:05:28.249981Z","iopub.execute_input":"2024-12-10T06:05:28.250692Z","iopub.status.idle":"2024-12-10T06:05:28.255990Z","shell.execute_reply.started":"2024-12-10T06:05:28.250644Z","shell.execute_reply":"2024-12-10T06:05:28.255129Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"test_inputs = tokenizer(test_sentences, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\ngenerated_ids = foundation_model.generate(input_ids=test_inputs['input_ids'], max_length=150)\n\ngenerated_responses = tokenizer.batch_decode(generated_ids[:, test_inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n\nfor i, sentence in enumerate(test_sentences):\n    print(f\"Input:\\n{sentence}\")\n    print(f\"Response:\\n{generated_responses[i]}\")\n    print(\"-\" * 50)\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T06:10:28.514461Z","iopub.execute_input":"2024-12-10T06:10:28.514845Z","iopub.status.idle":"2024-12-10T06:10:29.638634Z","shell.execute_reply.started":"2024-12-10T06:10:28.514783Z","shell.execute_reply":"2024-12-10T06:10:29.637735Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Input:\nQUESTION: What is the Iliad about? ANSWER:\nResponse:\n The Iliad by Homeric is a passage from The Iliad by Homeric, and is a passage from The Iliad by Homeric by Homeric.\n--------------------------------------------------\n\n\nInput:\nQUESTION: Who are the characters in the Iliad? ANSWER:\nResponse:\n Homeric, Homeric, and the Iliad by Homeric. Homeric is the only known writer who has written a passage of the Iliad by Homeric. Homeric is the only known writer who has written a passage by Homeric by Homeric. Homeric is the only known writer who has written a passage by Homeric by Homeric. Homeric is the only known writer who has written a passage by Homeric by Homeric by Homeric. Homeric is the only known writer who has written a passage by Homeric by Homeric by Homeric by Homeric. Homeric is the only known writer who has written a passage\n--------------------------------------------------\n\n\nInput:\nQUESTION:Who is Achilles? ANSWER:\nResponse:\n Achilles\n--------------------------------------------------\n\n\nInput:\nQUESTION:Who is Achilles' wife? ANSWER:\nResponse:\n Achilles' son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’s son’\n--------------------------------------------------\n\n\n","output_type":"stream"}],"execution_count":82},{"cell_type":"markdown","source":"## Conclusion\n    - In this notebook I showcased why foundation models pretrained on a large corpus are so important. If we just train the GPT model on the domain specific dataset we don't get the chatbot behavior. All downstream specialized tasks require pretraining so several teams have complete the undifferentiated pretraining for the community.\n    - Finetuned models are available on HuggingFace and are excellent ways to generate synthetic data for NLP tasks.\n    - My finetuned Iliad-GPT still didn't quite have the behavior I would like. Just look at the answer to 'Who is Achilles' Wife?'. In reality, it would require better data from the Q&A pipeline. If I had more compute, I could use models with larger context windows and provide better questions. Further, I could do more hyperparameter tuning with the extra compute.\n    - I think other finetuning frameworks like LoRA would have been useful to experiment with as well if time permitted.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}